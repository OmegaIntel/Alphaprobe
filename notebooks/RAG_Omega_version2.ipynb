{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "wTxbciOYVo7V"
      },
      "outputs": [],
      "source": [
        "# %pip install llama-index llama-index-core llama-parse openai llama_index.embeddings.huggingface -q\n",
        "# %pip install llama-index-llms-anthropic -q\n",
        "# %pip install llama-index-vector-stores-weaviate -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "import anthropic\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "from dotenv import load_dotenv\n",
        "from pdf2image import convert_from_path\n",
        "import base64\n",
        "import requests\n",
        "from llama_index.core import Document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "COMPARISON_FILE = 'claude-3-5-sonnet-20240620_qa.csv'\n",
        "PDF_LOCATION = 'IndustrySource/Misc/62 Healthcare and Social Assistance in the US Industry Report.pdf'\n",
        "DOC_ID = 'ibis-healthcare-social-assistance'\n",
        "MODEL_ID = 'gpt-4o-mini'\n",
        "QUESTION_COL = 'question'\n",
        "RESPONSE_COL = 'rag_model_response'\n",
        "NUM_QUESTIONS = -1\n",
        "PARSER = \"claude\" # \"claude\" or \"llama-parse\"\n",
        "CHUNK_SIZE = 600\n",
        "SPLITTER = \"sentence\"\n",
        "TOP_K = 3\n",
        "OUTPUT_FOLDER = f'./rag_outputs/{DOC_ID}'\n",
        "if not os.path.exists(OUTPUT_FOLDER):\n",
        "    os.makedirs(OUTPUT_FOLDER)\n",
        "OUTPUT_FILE = f'{OUTPUT_FOLDER}/output_{MODEL_ID}_{PARSER}_{CHUNK_SIZE}_{SPLITTER}_{TOP_K}.csv'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "df = pd.read_csv(COMPARISON_FILE)\n",
        "if NUM_QUESTIONS == -1:\n",
        "    dff = df.copy()\n",
        "else:\n",
        "    dff = df.head(NUM_QUESTIONS).copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "6gUzpLaeVpl5"
      },
      "outputs": [],
      "source": [
        "# llama-parse is async-first, running the async code in a notebook requires the use of nest_asyncio\n",
        "load_dotenv(\"/Users/mbajaj/.env\")\n",
        "# API access to llama-cloud\n",
        "os.environ[\"LLAMA_CLOUD_API_KEY\"] = os.getenv('LLAMA_CLOUD_API_KEY')\n",
        "# Using OpenAI API for embeddings/llms\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
        "os.environ[\"ANTHROPIC_API_KEY\"] = os.getenv('ANTHROPIC_API_KEY')\n",
        "os.environ[\"BRAINTRUST_API_KEY\"]=os.getenv('BRAINTRUST_API_KEY')\n",
        "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "IMAGE_PROMPT = \"\"\"\n",
        "Given an image of a page from a market research report, your task is to convert all the information on the page into markdown format, preserving the original structure and content.\n",
        "\n",
        "- Transcribe all text, including paragraphs and headings, verbatim from the page to markdown, maintaining the original format. Do not modify, omit, or add any text.\n",
        "- If the page includes numerical information with arrows, percentages etc, describe the text in full sentences in markdown format. For example, if there's a box with the text \"revenue of wine industry\" and an arrow pointing up saying \"10% (2015-2020)\", describe this as \"Revenue of wine industry increased by 10% from 2015 to 2020\" instead of just copying the text.\n",
        "- Do not explain any text that is clearly written in the page, including headings, subheadings, and paragraphs. Copy the text as it is.\n",
        "- If the text structure is unclear, use your best judgement to format it in markdown.\n",
        "- If the page contains tables, convert them into markdown table format and provide an explnation as well. Explain all the data that can be inferred from each table. For example, if a table shows sales data for different products, explain the sales trends and patterns with respect to each product. Try to provide as much detail as possible.\n",
        "- If the page includes a plot or graph, describe it objectively in markdown format. Explain all the details that can be inferred from the plot or graph. For example, if a plot shows sales trends over time, describe the sales trends and patterns observed. Provide a detailed explanation of the data represented in the plot or graph.\n",
        "- When explaining any component, understand the context of the entire page and be as specific as possible without any ambiguity. The position of the explanation should match the position of the component in the page.\n",
        "- The output should not contain personal opinions or biases. Do not add personal comments or any information not present on the page. Avoid referring to the page or the report - explain without reference.\n",
        "- Ensure no important information from the page is missed, as capturing all details is crucial.\n",
        "\"\"\"\n",
        "\n",
        "SYSTEM_MESSAGE = \"You are a profession converter that converts all the details in given image of page of a market research report to markdown format while preserving all the structure.\"\n",
        "\n",
        "FINAL_MESSAGE = \"Please describe the provided page in markdown format. Strictly follow the criteria mentioned above to describe each component of the page.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "def encode_image(image_path):\n",
        "    with open(image_path, \"rb\") as image_file:\n",
        "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
        "\n",
        "\n",
        "def request_claude_with_image(base64_image, model=\"claude-3-5-sonnet-20240620\"):\n",
        "    responded = False\n",
        "    num_tries = 0\n",
        "    failed = 0\n",
        "    \n",
        "    while not responded and num_tries < 5:\n",
        "        num_tries += 1\n",
        "\n",
        "        client = anthropic.Anthropic()\n",
        "        response = client.messages.create(\n",
        "            model=model,\n",
        "            max_tokens=5000,\n",
        "            system = SYSTEM_MESSAGE,\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": [\n",
        "                        {\"type\": \"text\", \"text\": IMAGE_PROMPT},\n",
        "                        {\n",
        "                            \"type\": \"image\",\n",
        "                            \"source\": {\n",
        "                                \"type\": \"base64\",\n",
        "                                \"media_type\": \"image/png\",\n",
        "                                \"data\": base64_image,\n",
        "                            },\n",
        "                        },\n",
        "                        {\"type\": \"text\", \"text\": FINAL_MESSAGE},\n",
        "                    ],\n",
        "                }\n",
        "            ],\n",
        "        )\n",
        "    \n",
        "        try:\n",
        "            response_txt = response.content[0].text\n",
        "            responded = True\n",
        "            return response_txt, failed\n",
        "        except:\n",
        "            failed += 1\n",
        "            continue\n",
        "    return None, failed\n",
        "\n",
        "\n",
        "def ClaudeParse(pdf_path, output_dir_path, model=\"claude-3-5-sonnet-20240620\"):\n",
        "    # check if outpu_pickle exists\n",
        "    output_pickle_path = os.path.join(output_dir_path, f\"_{model}_pages.pkl\")\n",
        "    if os.path.exists(output_pickle_path):\n",
        "        return pickle.load(open(output_pickle_path, \"rb\"))\n",
        "    \n",
        "    img_dir = os.path.join(output_dir_path, \"_imgs\")\n",
        "    if not os.path.exists(img_dir):\n",
        "        os.makedirs(img_dir)\n",
        "    \n",
        "    images = convert_from_path(pdf_path)\n",
        "    pages = []\n",
        "    skipped_pages = 0\n",
        "    average_failures = 0\n",
        "    # Iterate over the images\n",
        "    for i, image in enumerate(images):\n",
        "        # Define the path to save the image\n",
        "        image_path = os.path.join(img_dir, f'page_{i+1}.png')\n",
        "\n",
        "        # Save the image\n",
        "        image.save(image_path, 'PNG')\n",
        "        base64_image = encode_image(image_path)\n",
        "        response, failed = request_claude_with_image(base64_image, model)\n",
        "        average_failures += failed\n",
        "        if response is None:\n",
        "            print(f\"Failed to parse the page {i+1} after {failed} tries\")\n",
        "            response = \"\"\n",
        "            skipped_pages += 1\n",
        "\n",
        "        # convert the response to llama-index document\n",
        "        doc = Document(text=response, metadata={\"page_number\": i+1})\n",
        "        pages.append(doc)\n",
        "\n",
        "    print(f\"Average failures in calling claude API: {average_failures/len(images)}\")\n",
        "    print(f\"Skipped {skipped_pages} pages out of {len(images)}\")\n",
        "\n",
        "    output_text_path = os.path.join(output_dir_path, f\"_{model}_pages.txt\")\n",
        "\n",
        "    # save the pages as txt file for easier debugging\n",
        "    with open(output_text_path, \"w\") as f:\n",
        "        for page in pages:\n",
        "            #write page number\n",
        "            f.write(f\"***Page {page.metadata['page_number']}***\\n\\n\")\n",
        "            f.write(page.text)\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "    pickle.dump(pages, open(output_pickle_path, \"wb\"))\n",
        "    return pages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjWQnAYnVwhF",
        "outputId": "9907e56f-9e80-404e-b2d2-36f3ab269b24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "80\n"
          ]
        }
      ],
      "source": [
        "# from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "from llama_parse import LlamaParse\n",
        "if PARSER == \"llama-parse\":\n",
        "    documents = LlamaParse(result_type=\"markdown\").load_data(PDF_LOCATION)\n",
        "    print(len(documents))\n",
        "elif PARSER == \"claude\":\n",
        "    documents = ClaudeParse(PDF_LOCATION, OUTPUT_FOLDER)\n",
        "    print(len(documents))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "import uuid\n",
        "\n",
        "# generate unique index for multiple runs\n",
        "INDEX_NAME = ('X' + str(uuid.uuid4())).replace('-', '_')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "NfaNIvokpbKo"
      },
      "outputs": [],
      "source": [
        "import weaviate\n",
        "\n",
        "cluster_url = \"https://a0dlgmcaspopjrn2mtx4ha.c0.us-east1.gcp.weaviate.cloud\"\n",
        "api_key = \"7ZfUCibywHnzM0WKMPx7YevuN79nUtS4KJgT\"\n",
        "\n",
        "client = weaviate.connect_to_wcs(\n",
        "    cluster_url=cluster_url,\n",
        "    auth_credentials=weaviate.auth.AuthApiKey(api_key),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbN-rN7lsEy0"
      },
      "source": [
        "# weaviate vector database & llamaparse Integrated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# IBISWorld | Healthcare and Social Assistance in the US Mar 2024\n",
            "\n",
            "## Performance Snapshot\n",
            "\n",
            "### Revenue:\n",
            "\n",
            "The 2019-24 Revenue CAGR increased by 0.7%.\n",
            "\n",
            "### Revenue\n",
            "\n",
            "Revenue for 2024 is projected to be $3.6tr. The revenue growth rate for the period 2019-24 is 0.7%, while for 2024-29 it is expected to be 2.7%.\n",
            "\n",
            "### 2024 Revenue CAGR\n",
            "\n",
            "The 2024 Revenue CAGR shows an increase of 0.3%.\n",
            "\n",
            "### Revenue Volatility\n",
            "\n",
            "The Revenue Volatility is described as Moderate.\n",
            "\n",
            "### Revenue\n",
            "\n",
            "The graph presents the total value ($) and annual change from 2011 - 2029, including a 5-year outlook.\n",
            "\n",
            "The graph illustrates the Annual Revenue ($bn) and Change (%) from 2011 to 2029, with forecasted data from 2024 onwards. Key observations include:\n",
            "\n",
            "1. The Annual Revenue shows a general upward trend from 2011 to 2029.\n",
            "2. The Change (%) fluctuates significantly over the years, with notable peaks and troughs.\n",
            "3. For 2024, the Annual Revenue is projected to be $3554.9 billion, with a 0.3% change.\n",
            "4. The graph indicates a sharp decline in the change percentage around 2020-2021, possibly reflecting the impact of external events.\n",
            "5. After 2024, the revenue is forecasted to continue growing, while the change percentage shows moderate fluctuations.\n",
            "6. The highest positive change appears to occur around 2014-2015.\n",
            "7. The y-axis for Annual Revenue ranges from 0 to 5000 ($bn), while the Change (%) axis ranges from -5% to 7.5%.\n",
            "8. The forecasted period (2024-2029) shows a relatively stable growth in revenue with slight variations in the change percentage.\n",
            "\n",
            "Source: IBISWorld\n",
            "80\n"
          ]
        }
      ],
      "source": [
        "print(documents[15].text)\n",
        "print(len(documents))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "97\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core.schema import TextNode\n",
        "\n",
        "nodes = []\n",
        "if SPLITTER == \"sentence\":\n",
        "\n",
        "    splitter = SentenceSplitter(chunk_size=CHUNK_SIZE)\n",
        "    for idx, doc in enumerate(documents):\n",
        "        chunks = splitter.split_text(doc.text)\n",
        "        # create nodes with metadata\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            node = TextNode(text=chunk, metadata={'page_number': idx+1})\n",
        "            nodes.append(node)\n",
        "\n",
        "\n",
        "print(len(nodes))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "hVThHH30qRoS"
      },
      "outputs": [],
      "source": [
        "from llama_index.vector_stores.weaviate import WeaviateVectorStore\n",
        "\n",
        "vector_store = WeaviateVectorStore(\n",
        "    weaviate_client=client, index_name=INDEX_NAME\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "esw4x1faqmmS"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "vector_index = VectorStoreIndex(nodes, vector_store = vector_store)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "LMoIlj0qq-Wn"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "llm = OpenAI(model=MODEL_ID, api_key = OPENAI_API_KEY)\n",
        "query_engine = vector_index.as_query_engine(similarity_top_k=TOP_K, llm=llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((213, 7),\n",
              " './RagOutputs/ibis-healthcare-social-assistance_gpt-4o-mini_claude_600_sentence_3/output.csv')"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dff.shape, OUTPUT_FILE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = []\n",
        "references = []\n",
        "contexts = []\n",
        "for question in dff[QUESTION_COL]:\n",
        "    response = query_engine.query(question)\n",
        "    result.append((response.response))\n",
        "    metadata = response.metadata\n",
        "    refs = []\n",
        "    for m in metadata.values():\n",
        "        refs.append(m['page_number'])\n",
        "    references.append(refs)\n",
        "    q_contexts = []\n",
        "    for n in response.source_nodes:\n",
        "        q_contexts.append(n.text)\n",
        "    contexts.append(q_contexts)\n",
        "\n",
        "dff[RESPONSE_COL] = result\n",
        "dff['references'] = references\n",
        "dff['context'] = contexts\n",
        "dff.to_csv(OUTPUT_FILE, index=False)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
