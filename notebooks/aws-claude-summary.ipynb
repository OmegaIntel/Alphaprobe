{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rely on LLM's to extract info from docs\n",
    "\n",
    "Will save them in JSON for future reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import boto3\n",
    "import json\n",
    "import dateparser\n",
    "from glob import glob\n",
    "# from rag.basic_retrieval import file_id\n",
    "# from cachier import cachier\n",
    "from typing import List\n",
    "\n",
    "\n",
    "from Templates.aws_markdown_template import TEMPLATE as MARKDOWN_TEMPLATE\n",
    "from Templates.aws_templates_common import build_aws_template\n",
    "\n",
    "from loading_utils import get_initial_pages\n",
    "\n",
    "from IPython.display import Markdown\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF_LOCATION = 'IndustrySource/Misc/3D Printer Manufacturing in the US.pdf'\n",
    "PDF_LOCATION = 'IndustrySource/Misc/HVAC%20Service%20Franchises%20in%20the%20US.pdf'\n",
    "DOC_ID = PDF_LOCATION.split('/')[-1].split('.')[0].lower().replace(' ', '-')\n",
    "OUTPUT_FOLDER = f'./rag_outputs/{DOC_ID}'\n",
    "if not os.path.exists(OUTPUT_FOLDER):\n",
    "    os.makedirs(OUTPUT_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS_REGION_NAME = 'us-west-2'\n",
    "AWS_REGION_NAME = 'us-east-1'\n",
    "\n",
    "aws_access_key_id = os.getenv('AWS_ACCESS_KEY_ID')\n",
    "aws_secret_access_key = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "\n",
    "# https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-runtime.html\n",
    "bedrock = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    "    region_name=AWS_REGION_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARKDOWN_PROMPT = \"\"\"\n",
    "We are sequentially converting a pdf document page by page to markdown format. You are an expert in converting the given PDF content to a Markdown representation.\n",
    "\n",
    "Follow these instructions to complete the task:\n",
    "- Infer the headings and subheadings of the given content with their levels from the appearance and semantic context. Generally, the larger font size, more visible color, and boldface indicate a lower level of the heading. For example, level 1 headings are expected to be more prominent than level 2 headings.\n",
    "- Depending on the level of the section, you use an appropriate number of hash signs (#) to mark their headers in markdown format. # for level 1, ## for level 2, ### for level 3 and so on.\n",
    "- The provided content may start from any page of the document. So the heading at first does not necessarily mean a level 1 heading.\n",
    "- Do not insert any new content. Just convert the existing content to Markdown format while keeping the structure.\n",
    "- Text is converted as it is. If the text is present in conflicting format that can be confusing, please interpret the text correctly and convert it to markdown.\n",
    "- For visuals such as graphs, plots and figures, interpret them, be objective and explain the interpretation of data in detail with numbers and use that explanation in place of the visuals. Explanation provided should capture all the data insights that can be inferred from the figure. Use your best judgement to interpret the visuals.\n",
    "- For tables, convert them to markdown table format without any explanation.\n",
    "- The output should be in markdown format. Do not modify any content. \n",
    "Next is the pdf content:\\n\\n\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_pdf_part(filename: str) -> dict:\n",
    "    \"\"\"This works best and parses quickly.\"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        content = f.read()\n",
    "        return {\n",
    "            \"document\": {\n",
    "                \"format\": \"pdf\",\n",
    "                \"name\": 'document',\n",
    "                \"source\": {\n",
    "                    \"bytes\": content\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "def response_to_template(filename: str, template: dict, prompt: str) -> dict:\n",
    "    initial_message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"text\": prompt,\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    initial_message['content'].append(get_raw_pdf_part(filename))\n",
    "    \n",
    "\n",
    "    tool_list = [{\n",
    "        \"toolSpec\": template\n",
    "    }]\n",
    "    model_id = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "    response = bedrock.converse(\n",
    "        modelId=model_id,\n",
    "        # modelId=\"meta.llama3-1-405b-instruct-v1:0\",\n",
    "        messages=[initial_message],\n",
    "        inferenceConfig={\n",
    "            \"temperature\": 0\n",
    "        },\n",
    "        toolConfig={\n",
    "            \"tools\": tool_list,\n",
    "            \"toolChoice\": {\n",
    "                \"tool\": {\n",
    "                    \"name\": \"info_extract\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    core_response = response['output']['message']['content'][0]['toolUse']['input']\n",
    "    if 'properties' in core_response:\n",
    "        core_response: dict = core_response['properties']\n",
    "    for k, v in core_response.items():\n",
    "        if isinstance(v, str) and v[0] in '{[' and v[-1] in ']}':\n",
    "            try:\n",
    "                core_response[k] = json.loads(v)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    return core_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# @cachier(hash_func=filename_template_hash)\n",
    "def info_from_doc_template(filename: str, template: dict, prompt: str, **kwargs) -> dict:\n",
    "    \"\"\"Populate the separate templates and merge the result.\"\"\"\n",
    "\n",
    "    template_parts = template['data']\n",
    "    full_templates = build_aws_template(template_parts)\n",
    "    results = [response_to_template(filename, part, prompt, **kwargs) for part in full_templates]\n",
    "\n",
    "    total = {}\n",
    "    for result in results:\n",
    "        total.update(result)\n",
    "\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_markdown(filename: str, first_page: int, last_page: int) -> dict:\n",
    "    pages_filename = get_initial_pages(filename, pmin=first_page, pmax=last_page+1)\n",
    "    result = info_from_doc_template(filename=pages_filename, template=MARKDOWN_TEMPLATE, prompt=MARKDOWN_PROMPT)\n",
    "    os.remove(pages_filename)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "import time\n",
    "\n",
    "def get_number_of_pages(filename: str) -> int:\n",
    "    with open(filename, 'rb') as f:\n",
    "        inputpdf = PdfReader(f)\n",
    "        return len(inputpdf.pages)\n",
    "\n",
    "def sequentially_process_pdf(filename, np=1):\n",
    "    # find total number of pages in the pdf document\n",
    "    total_pages = get_number_of_pages(filename)\n",
    "\n",
    "    results = []\n",
    "    skipped_pages = []\n",
    "    for i in range(0, total_pages, np):\n",
    "        print(f\"Processing pages {i} to {i+np}\")\n",
    "        pages_filename = get_initial_pages(filename, pmin=i, pmax=i+np)\n",
    "        failures = 0\n",
    "        success = False\n",
    "        result = None\n",
    "        while (not success) and (failures < 3):\n",
    "            try:\n",
    "                result = info_from_doc_template(filename=pages_filename, template=MARKDOWN_TEMPLATE, prompt=MARKDOWN_PROMPT)\n",
    "                success = True\n",
    "                time.sleep(5)\n",
    "            except:\n",
    "                print(f\"Error processing page {i+1}\")\n",
    "                failures += 1\n",
    "                print(f\"Retrying in 60 seconds.\")\n",
    "                time.sleep(60)\n",
    "                if failures == 3:\n",
    "                    result = {\n",
    "                        'markdown': \"**skipped**\",\n",
    "                    }\n",
    "                    print(f\"Failed to process page {i+1} after 3 attempts.\")\n",
    "                    skipped_pages.append(i+1)\n",
    "\n",
    "        results.append(result)\n",
    "        os.remove(pages_filename)\n",
    "    print(f\" Had to skip pages: {len(skipped_pages)}\")\n",
    "    return results, skipped_pages\n",
    "\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing pages 0 to 1\n",
      "Processing pages 1 to 2\n",
      "Error processing page 2\n",
      "Retrying in 60 seconds.\n",
      "Processing pages 2 to 3\n",
      "Error processing page 3\n",
      "Retrying in 60 seconds.\n",
      "Processing pages 3 to 4\n",
      "Error processing page 4\n",
      "Retrying in 60 seconds.\n",
      "Processing pages 4 to 5\n",
      "Error processing page 5\n",
      "Retrying in 60 seconds.\n",
      "Processing pages 5 to 6\n",
      "Error processing page 6\n",
      "Retrying in 60 seconds.\n",
      "Processing pages 6 to 7\n",
      "Error processing page 7\n",
      "Retrying in 60 seconds.\n",
      "Processing pages 7 to 8\n",
      "Error processing page 8\n",
      "Retrying in 60 seconds.\n",
      "Processing pages 8 to 9\n",
      "Error processing page 9\n",
      "Retrying in 60 seconds.\n",
      "Processing pages 9 to 10\n",
      "Error processing page 10\n",
      "Retrying in 60 seconds.\n",
      "Processing pages 10 to 11\n",
      "Error processing page 11\n",
      "Retrying in 60 seconds.\n",
      "Processing pages 11 to 12\n",
      "Error processing page 12\n",
      "Retrying in 60 seconds.\n",
      "Processing pages 12 to 13\n",
      "Error processing page 13\n",
      "Retrying in 60 seconds.\n",
      "Processing pages 13 to 14\n",
      "Error processing page 14\n",
      "Retrying in 60 seconds.\n",
      "Processing pages 14 to 15\n"
     ]
    }
   ],
   "source": [
    "file_path = PDF_LOCATION\n",
    "\n",
    "results, skipped_pages = sequentially_process_pdf(file_path, np=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results[0]['markdown'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "import pickle\n",
    "model = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "\n",
    "output_pickle_path = os.path.join(OUTPUT_FOLDER, f\"_{model}_pages.pkl\")\n",
    "\n",
    "documents = []\n",
    "for i, response in enumerate(results):\n",
    "    doc = Document(text=results[i]['markdown'], metadata={\"page_number\": i+1})\n",
    "    documents.append(doc)\n",
    "parsed_result = {\"parsed_documents\": documents, \"skipped_pages\": skipped_pages}\n",
    "pickle.dump(parsed_result, open(output_pickle_path, 'wb'))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filenames = [\n",
    "#     'IndustrySource/Misc/3D Printer Manufacturing in the US.pdf'\n",
    "# ]\n",
    "# markdown = [extract_markdown(filename, 45, 48) for filename in filenames[:5]]\n",
    "# markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2 : Load documents and do post processing to correct the heading structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "import pickle\n",
    "\n",
    "PDF_LOCATION = 'IndustrySource/Misc/3D Printer Manufacturing in the US.pdf'\n",
    "DOC_ID = PDF_LOCATION.split('/')[-1].split('.')[0].lower().replace(' ', '-')\n",
    "OUTPUT_FOLDER = f'./rag_outputs/{DOC_ID}'\n",
    "\n",
    "\n",
    "model = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "\n",
    "output_pickle_path = os.path.join(OUTPUT_FOLDER, f\"_{model}_pages.pkl\")\n",
    "\n",
    "# load pickle file if it exists\n",
    "if os.path.exists(output_pickle_path):\n",
    "    with open(output_pickle_path, 'rb') as f:\n",
    "        parsed_result = pickle.load(f)\n",
    "\n",
    "documents = parsed_result['parsed_documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [document.text for document in documents]\n",
    "# iterate over docs and see if the first sentence contains the title, then remove the whole line from the text\n",
    "all_sentences = []\n",
    "for i, doc in enumerate(docs):\n",
    "    sentences = doc.split(\"\\n\")\n",
    "    all_sentences.append(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "headings = []\n",
    "for i, sentences in enumerate(all_sentences):\n",
    "    for j, sentence in enumerate(sentences):\n",
    "        # sentence is a heading if it starts with # or ## or ### or #### or #####\n",
    "        if sentence.startswith(\"#\"):\n",
    "            headings.append({\"text\": sentence, \"page\": i+1, \"sentence_number\": j+1})\n",
    "\n",
    "print(len(headings))\n",
    "headings[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADINGS_PROMPT = ''' You will be given a text containing headings and subheadings parsed from the market research report of a single industry in markdown format. \n",
    "However, the issue is that these subheadings were parsed one page at a time. So this makes it possible that the structure of the document is not preserved.\n",
    "The heading 2 in the document might be parsed as heading 1 if it is the first heading on the page. Similarly, all the following subheadings on the same page might be parsed as different heading levels.\n",
    "However, the order in wich the headings appear is preserved from top to bottom of each page. So you don't have to worry about the order of the headings.\n",
    "Other issue is that parsed headings/ subheadings may accidentally include date or page number or report title that might have been present as a header or footer in the document. Some headings might be repeated as well so you need to remove the repetitions.\n",
    "Your task is to identify the correct structure of the document by taking into account the semantic meanings of the headings and subheadings.\n",
    "Your input will be a text file with each line starting with <page_number, line_number> of the heading/ subheading followed by the heading or subheading text starting with # or ## or ### or #### or ##### denoting heading 1, heading 2, heading 3, heading 4 and heading 5 respectively.\n",
    "Your output will be a text file with each line containing the same <page_number, line_number> as the one in input followed by the heading or subheading text with the correct heading level denoted by # or ## or ### or #### or #####. \n",
    "You must not change the page number and line number of the respective heading or subheading. You may entrirely remove the heading or subheading if it is a repetition or report title or date that was incorrectly included as a heading. Otherwise, you should correct the heading level if you think it was parsed incorrectly.\n",
    "To make it easier for you, here are the expected major sections (level 1) of the document: Industry at a Glance, Supply Chain, Competitive Landscape, Costs & Operations, Questions for Owners, Datatables & Glossary\n",
    "'''\n",
    "\n",
    "SYSTEM_MESSAGE = \"You are an expert in identifying the correct structure of a document by taking into account the page numbers, line numbers and the semantic meanings of the headings and subheadings.\"\n",
    "\n",
    "FINAL_MESSAGE = \"Please identify the correct structure of the document by taking into account the page numbers, line numbers and the semantic meanings of the headings and subheadings. Output the headings with the correct heading level denoted by # or ## or ### or #### or #####.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\n",
    "for heading in headings:\n",
    "    input_text += f\"<{heading['page']}, {heading['sentence_number']}> {heading['text']}\\n\"\n",
    "input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "\n",
    "from llama_index.core import Document\n",
    "\n",
    "client = anthropic.Anthropic()\n",
    "model = \"claude-3-5-sonnet-20240620\"\n",
    "\n",
    "response = client.messages.create(\n",
    "    model=model,\n",
    "    max_tokens=5000,\n",
    "    system = SYSTEM_MESSAGE,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": HEADINGS_PROMPT},\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": input_text\n",
    "                },\n",
    "                {\"type\": \"text\", \"text\": FINAL_MESSAGE},\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = response.content[0].text\n",
    "output_list = output.split(\"\\n\")\n",
    "# filter output list that do not start with <\n",
    "output_list = [line for line in output_list if line.lstrip().startswith(\"<\")]\n",
    "print(len(output_list))\n",
    "output_list[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, heading, level, page, line_number, refined_output_idx):\n",
    "        self.heading = heading\n",
    "        self.text = None\n",
    "        self.level = level\n",
    "        self.page = page\n",
    "        self.line_number = line_number\n",
    "        self.children = []\n",
    "        self.parent = None\n",
    "        self.self_index = None\n",
    "        self.is_partial_node = False\n",
    "        self.is_partial_node_parent = False\n",
    "        self.text_size = 0\n",
    "        self.refined_output_idx = refined_output_idx  \n",
    "        self.embedding_text = None\n",
    "        # generate a unique id for each node\n",
    "        uid = str(uuid.uuid4())[:3]\n",
    "        self.id_ = f\"{self.page}_{self.line_number}_{self.level}_{uid}\"\n",
    "    \n",
    "    def add_child(self, child):\n",
    "        child.self_index = len(self.children)\n",
    "        self.children.append(child)\n",
    "        child.parent = self\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"self index: {self.self_index} level: {self.level} loc: <{self.page, self.line_number}> content: {self.heading}\"\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"self index: {self.self_index} level: {self.level} loc: <{self.page, self.line_number}> content: {self.heading}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a tree structure of the headings and subheadings from output_list such as level 1 is parent of level 2 and so on\n",
    "\n",
    "def get_level(heading):\n",
    "    text = heading.lstrip()\n",
    "    level = 0\n",
    "    for char in text:\n",
    "        if char == \"#\":\n",
    "            level += 1\n",
    "        else:\n",
    "            break\n",
    "    return level\n",
    "\n",
    "def get_fields_from_line(text):\n",
    "    level = 0\n",
    "    line = text.lstrip()\n",
    "    if line.startswith(\"<\"):\n",
    "        prefix = (line.split(\"> \")[0]).split(\"<\")[1]\n",
    "        page, line_number = int(prefix.split(\",\")[0]), int(prefix.split(\",\")[1])\n",
    "        heading = line.split(\"> \")[1]\n",
    "        level = get_level(heading)\n",
    "        return page, line_number, heading, level\n",
    "    else:\n",
    "        return None, None, None, level\n",
    "\n",
    "\n",
    "root = Node(heading=\"\", level=0, page=0, line_number=0, refined_output_idx=-1)\n",
    "parent = root\n",
    "# construct the tree\n",
    "# TODO: take page, line_number from the input instead of output to avoid any issues\n",
    "\n",
    "refined_output = []\n",
    "for i in range(len(output_list)):\n",
    "    page, line_number, heading, level = get_fields_from_line(output_list[i])\n",
    "    if level == 0:\n",
    "        continue\n",
    "    if level > parent.level:\n",
    "        child = Node(heading, level, page, line_number, len(refined_output))\n",
    "        parent.add_child(child)\n",
    "        parent = child\n",
    "    elif level == parent.level:\n",
    "        child = Node(heading, level, page, line_number, len(refined_output))\n",
    "        parent.parent.add_child(child)\n",
    "        parent = child\n",
    "    else: # level < parent.level\n",
    "        while level < parent.level:\n",
    "            parent = parent.parent\n",
    "        child = Node(heading, level, page, line_number, len(refined_output))\n",
    "        if level == parent.level:\n",
    "            parent.parent.add_child(child)\n",
    "        else:\n",
    "            parent.add_child(child)\n",
    "        parent = child\n",
    "    refined_output.append((page, line_number, heading, level))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_text(node):\n",
    "    if node.text is not None:\n",
    "        return node.text\n",
    "    \n",
    "    if node.level == 0:\n",
    "        return \"\"\n",
    "    page = node.page\n",
    "    line_number = node.line_number\n",
    "    # check if there is any heading after this\n",
    "    if len(refined_output) > node.refined_output_idx + 1:\n",
    "        next_heading = refined_output[node.refined_output_idx + 1]\n",
    "        next_page, next_line_number, next_heading_text, next_level = next_heading\n",
    "        # get text between this heading and next heading using all_sentences\n",
    "        text = \"\"\n",
    "        if page == next_page:\n",
    "            if line_number + 1 < next_line_number:\n",
    "                text += \"\\n\".join(all_sentences[page-1][line_number:next_line_number-1])\n",
    "        else:\n",
    "            text += \"\\n\".join(all_sentences[page-1][line_number:])\n",
    "            for p in range(page+1, next_page):\n",
    "                text += \"\\n\".join(all_sentences[p-1])\n",
    "            text += \"\\n\".join(all_sentences[next_page-1][:next_line_number-1])\n",
    "        return text\n",
    "    else:\n",
    "        # get text from this heading to the end of the page\n",
    "        text = \"\\n\".join(all_sentences[page-1][line_number:])\n",
    "        for p in range(page+1, len(all_sentences)):\n",
    "            text += \"\\n\".join(all_sentences[p-1])\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split text to chunk sizes less than 1024\n",
    "\n",
    "def split_text_to_chunk_size(text, target_size=1024):\n",
    "    text_sentences = text.split(\"\\n\")\n",
    "    chunks = []\n",
    "    current_chunk_len = 0\n",
    "    current_chunk = \"\"\n",
    "    for sentence in text_sentences:\n",
    "        sentence_len = len(sentence.split(\" \"))\n",
    "        if current_chunk_len + sentence_len < target_size:\n",
    "            current_chunk += sentence + \"\\n\"\n",
    "            current_chunk_len += sentence_len\n",
    "        else:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = sentence + \"\\n\"\n",
    "            current_chunk_len = sentence_len\n",
    "    if current_chunk_len > 0:\n",
    "        chunks.append(current_chunk)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's traverse the tree and see if we need to split some nodes into multiple nodes due to text size more than selected chunk size\n",
    "\n",
    "# do pre-order traversal \n",
    "\n",
    "\n",
    "\n",
    "def set_text_and_split_necessary_nodes(node, splitter_func, chunk_size=500):\n",
    "    if node.text is None:\n",
    "        text = get_node_text(node)\n",
    "        text_size = len(text.split(\" \"))\n",
    "    else:\n",
    "        text = node.text\n",
    "        text_size = node.text_size\n",
    "    \n",
    "    if text_size > chunk_size:\n",
    "        print(\"Splitting node: \", node.refined_output_idx)\n",
    "        # split the node into multiple nodes\n",
    "        text_chunks = splitter_func(text, target_size=chunk_size)\n",
    "        # now we replace original node with dummy node which has heading but the text is empty. text will be added to children\n",
    "        node.text = \"\"\n",
    "        node.is_partial_node_parent = True\n",
    "        node.text_size = 0\n",
    "        # store the children of this node and remove them from the node\n",
    "        original_children = node.children\n",
    "        node.children = []\n",
    "        print(\"Node children: \", len(original_children))\n",
    "        print(\"Splitting into: \", len(text_chunks))\n",
    "\n",
    "        for i in range(len(text_chunks)):\n",
    "            child = Node(heading=\"\", level=node.level, page=node.page, line_number=node.line_number, refined_output_idx=node.refined_output_idx)\n",
    "            child.text = text_chunks[i]\n",
    "            child.text_size = len(text_chunks[i].split(\" \"))\n",
    "            # add this child to the parent\n",
    "            node.add_child(child)\n",
    "            child.is_partial_node = True\n",
    "            # check if this is the last child, it last child add the original children to this child\n",
    "            if i == len(text_chunks) - 1:\n",
    "                for original_child in original_children:\n",
    "                    child.add_child(original_child)\n",
    "    else:\n",
    "        node.text = text\n",
    "        node.text_size = text_size\n",
    "    \n",
    "    if len(node.children) > 0:\n",
    "        for child in node.children:\n",
    "            set_text_and_split_necessary_nodes(child, splitter_func, chunk_size)\n",
    "    return\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's process the tree and split the nodes with text size more than 500 words into multiple nodes\n",
    "size_based_splitter = split_text_to_chunk_size\n",
    "set_text_and_split_necessary_nodes(root, splitter_func=size_based_splitter, chunk_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_structure_path = os.path.join(OUTPUT_FOLDER, f\"tree_structure.pkl\")\n",
    "# save the tree structure\n",
    "pickle.dump(root, open(tree_structure_path, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3: Extract info from respective section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the tree structure \n",
    "import pickle\n",
    "tree_structure_path = os.path.join(OUTPUT_FOLDER, f\"tree_structure.pkl\")\n",
    "def load_tree_structure(input_path):\n",
    "    with open(input_path, \"rb\") as file:\n",
    "        return pickle.load(tree_structure_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only for debug purpose\n",
    "# print the tree structure to see there is no loop in the tree and count the nodes\n",
    "\n",
    "def print_tree_structure(node):\n",
    "    print(node)\n",
    "    if len(node.children) > 0:\n",
    "        bottom_nodes = 0\n",
    "        for child in node.children:\n",
    "            bottom_nodes += print_tree_structure(child)\n",
    "        return bottom_nodes + 1\n",
    "    return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_structure_path = os.path.join(OUTPUT_FOLDER, f\"tree_structure.pkl\")\n",
    "root = load_tree_structure(tree_structure_path)\n",
    "print(root.children)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INFO_EXTRACTION_PROMPT = \"\"\"\n",
    "You are an expert in extracting market and financial data from documents.\n",
    "Use the given tool to extract essential data from text in the enclosed document. Do not make any assumptions or add any information that is not present in the text.\n",
    "\n",
    "Return the result in JSON format. Do not use non-JSON tags. If some numeric data is not present in the text, simply output the number 101 as an answer where numeric data is expected.\n",
    "For titles and names, limit the output to 20 words. For descriptions and key points, limit the output to 50 words.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response_to_text(content_text: str, template: dict, main_prompt: str, system_prompt: str, final_prompt: str) -> dict:\n",
    "    initial_message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"text\": main_prompt,\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    initial_message['content'].append({\"text\": content_text})\n",
    "    if final_prompt is not None:\n",
    "        initial_message['content'].append({\"text\": final_prompt})\n",
    "    \n",
    "\n",
    "    tool_list = [{\n",
    "        \"toolSpec\": template\n",
    "    }]\n",
    "    # model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "    model_id = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "    response = bedrock.converse(\n",
    "        modelId=model_id,\n",
    "        # modelId=\"meta.llama3-1-405b-instruct-v1:0\",\n",
    "        messages=[initial_message],\n",
    "        # system = system_prompt,\n",
    "        inferenceConfig={\n",
    "            \"temperature\": 0,\n",
    "        },\n",
    "        toolConfig={\n",
    "            \"tools\": tool_list,\n",
    "            \"toolChoice\": {\n",
    "                \"tool\": {\n",
    "                    \"name\": \"info_extract\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    core_response = response['output']['message']['content'][0]['toolUse']['input']\n",
    "    if 'properties' in core_response:\n",
    "        core_response: dict = core_response['properties']\n",
    "    for k, v in core_response.items():\n",
    "        if isinstance(v, str) and v[0] in '{[' and v[-1] in ']}':\n",
    "            try:\n",
    "                core_response[k] = json.loads(v)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    return core_response, response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_section_text(node):\n",
    "    text = node.heading + \"\\n\"\n",
    "    if node.text:\n",
    "        text += node.text + \"\\n\"\n",
    "    for child in node.children:\n",
    "        text += get_section_text(child)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Templates.ibis_aws_summary_template_all import TEMPLATE as IBIS_SUMMARY_TEMPLATE\n",
    "template_parts = IBIS_SUMMARY_TEMPLATE['data']\n",
    "full_templates = build_aws_template(template_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_templates[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "\n",
    "def extract_info_for_section(node, template, main_prompt, system_prompt, final_prompt):\n",
    "    section_text = get_section_text(node)\n",
    "    num_failed = 0\n",
    "    result = None\n",
    "    while num_failed < 5:\n",
    "        try:\n",
    "            result = response_to_text(section_text, template, main_prompt, system_prompt, final_prompt)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            num_failed += 1\n",
    "            print(f\"Error: {e}\")\n",
    "            print(f\"Failed {num_failed} times. Sleeping for 60 seconds.\")\n",
    "            time.sleep(60)\n",
    "            continue\n",
    "    return result\n",
    "\n",
    "def extract_info_for_all_sections(root, full_templates):\n",
    "    section_summaries = []\n",
    "    raw_responses = []\n",
    "\n",
    "    for idx, child in enumerate(root.children):\n",
    "        if idx >= len(full_templates):\n",
    "            break\n",
    "        section_summary, raw_response = extract_info_for_section(child, full_templates[idx], INFO_EXTRACTION_PROMPT, None, None)\n",
    "        section_summaries.append(section_summary)\n",
    "        raw_responses.append(raw_response)\n",
    "        print(f\"Extracted for section {section_name}\")\n",
    "        print(\"Sleeping for 60 seconds.\")\n",
    "    return section_summaries, raw_responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "section_summaries, raw_responses = extract_info_for_all_sections(root, full_templates)\n",
    "section_results = {\"section_summaries\": section_summaries, \"raw_responses\": raw_responses}\n",
    "\n",
    "pickle.dump(section_results, open(f\"{OUTPUT_FOLDER}/section_summaries.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from build_markdown_report import build_markdown_report_func\n",
    "\n",
    "report_md = build_markdown_report_func(section_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(report_md)\n",
    "# save the markdown report\n",
    "with open(f\"{OUTPUT_FOLDER}/summary_report.md\", \"w\") as file:\n",
    "    file.write(report_md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Templates.build_markdown_report import report_order\n",
    "import json\n",
    "json_path = f\"{OUTPUT_FOLDER}/section_summaries.json\"\n",
    "with open(json_path, \"w\") as f:\n",
    "    json.dump(section_summaries, f, indent=4)\n",
    "\n",
    "report_order_json_path = f\"{OUTPUT_FOLDER}/report_order.json\"\n",
    "with open(report_order_json_path, \"w\") as f:\n",
    "    json.dump(report_order, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "probe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
