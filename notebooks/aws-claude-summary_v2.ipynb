{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rely on LLM's to extract info from docs\n",
    "\n",
    "Will save them in JSON for future reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import boto3\n",
    "import json\n",
    "import dateparser\n",
    "from glob import glob\n",
    "# from rag.basic_retrieval import file_id\n",
    "# from cachier import cachier\n",
    "from typing import List\n",
    "\n",
    "\n",
    "from Templates.aws_markdown_template import TEMPLATE as MARKDOWN_TEMPLATE\n",
    "from Templates.aws_templates_common import build_aws_template\n",
    "\n",
    "from IPython.display import Markdown\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF_LOCATION = 'IndustrySource/Misc/3D Printer Manufacturing in the US.pdf'\n",
    "# PDF_LOCATION = 'IndustrySource/Misc/HVAC%20Service%20Franchises%20in%20the%20US.pdf'\n",
    "\n",
    "PDF_LOCATION = 'IndustrySource/HVAC/HVAC Service Franchises in the US.pdf'\n",
    "DOC_ID = PDF_LOCATION.split('/')[-1].split('.')[0].lower().replace(' ', '-')\n",
    "OUTPUT_FOLDER = f'./rag_outputs/{DOC_ID}'\n",
    "if not os.path.exists(OUTPUT_FOLDER):\n",
    "    os.makedirs(OUTPUT_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS_REGION_NAME = 'us-west-2'\n",
    "AWS_REGION_NAME = 'us-east-1'\n",
    "\n",
    "aws_access_key_id = os.getenv('AWS_ACCESS_KEY_ID')\n",
    "aws_secret_access_key = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "\n",
    "# https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-runtime.html\n",
    "bedrock = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    "    region_name=AWS_REGION_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "import time\n",
    "\n",
    "def get_number_of_pages(filename: str) -> int:\n",
    "    with open(filename, 'rb') as f:\n",
    "        inputpdf = PdfReader(f)\n",
    "        return len(inputpdf.pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARKDOWN_PROMPT = \"\"\"\n",
    "We are sequentially converting a pdf document page by page to markdown format. You are an expert in converting the given PDF content to a Markdown representation.\n",
    "\n",
    "Follow these instructions to complete the task:\n",
    "- Infer the headings and subheadings of the given content with their levels from the appearance and semantic context. Generally, the larger font size, more visible color, and boldface indicate a lower level of the heading. For example, level 1 headings are expected to be more prominent than level 2 headings.\n",
    "- Depending on the level of the section, you use an appropriate number of hash signs (#) to mark their headers in markdown format. # for level 1, ## for level 2, ### for level 3 and so on.\n",
    "- The provided content may start from any page of the document. So the heading at first does not necessarily mean a level 1 heading.\n",
    "- Do not insert any new content. Just convert the existing content to Markdown format while keeping the structure.\n",
    "- Text is converted as it is. If the text is present in conflicting format that can be confusing, please interpret the text correctly and convert it to markdown.\n",
    "- For visuals such as graphs, plots and figures, interpret them, be objective and explain the interpretation of data in detail with numbers and use that explanation in place of the visuals. Explanation provided should capture all the data insights that can be inferred from the figure. Use your best judgement to interpret the visuals.\n",
    "- For tables, convert them to markdown table format without any explanation.\n",
    "- The output should be in markdown format. Do not modify any content. \n",
    "Next is the pdf content:\\n\\n\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_pdf_part(filename: str) -> dict:\n",
    "    \"\"\"This works best and parses quickly.\"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        content = f.read()\n",
    "        return {\n",
    "            \"document\": {\n",
    "                \"format\": \"pdf\",\n",
    "                \"name\": 'document',\n",
    "                \"source\": {\n",
    "                    \"bytes\": content\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "def response_to_template(filename: str, template: dict, prompt: str) -> dict:\n",
    "    initial_message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"text\": prompt,\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    initial_message['content'].append(get_raw_pdf_part(filename))\n",
    "    \n",
    "\n",
    "    tool_list = [{\n",
    "        \"toolSpec\": template\n",
    "    }]\n",
    "    model_id = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "    response = bedrock.converse(\n",
    "        modelId=model_id,\n",
    "        # modelId=\"meta.llama3-1-405b-instruct-v1:0\",\n",
    "        messages=[initial_message],\n",
    "        inferenceConfig={\n",
    "            \"temperature\": 0\n",
    "        },\n",
    "        toolConfig={\n",
    "            \"tools\": tool_list,\n",
    "            \"toolChoice\": {\n",
    "                \"tool\": {\n",
    "                    \"name\": \"info_extract\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    core_response = response['output']['message']['content'][0]['toolUse']['input']\n",
    "    if 'properties' in core_response:\n",
    "        core_response: dict = core_response['properties']\n",
    "    for k, v in core_response.items():\n",
    "        if isinstance(v, str) and v[0] in '{[' and v[-1] in ']}':\n",
    "            try:\n",
    "                core_response[k] = json.loads(v)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    return core_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# @cachier(hash_func=filename_template_hash)\n",
    "def info_from_doc_template(filename: str, template: dict, prompt: str, **kwargs) -> dict:\n",
    "    \"\"\"Populate the separate templates and merge the result.\"\"\"\n",
    "\n",
    "    template_parts = template['data']\n",
    "    full_templates = build_aws_template(template_parts)\n",
    "    results = [response_to_template(filename, part, prompt, **kwargs) for part in full_templates]\n",
    "\n",
    "    total = {}\n",
    "    for result in results:\n",
    "        total.update(result)\n",
    "\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_markdown(filename: str, first_page: int, last_page: int) -> dict:\n",
    "    pages_filename = get_initial_pages(filename, pmin=first_page, pmax=last_page+1)\n",
    "    result = info_from_doc_template(filename=pages_filename, template=MARKDOWN_TEMPLATE, prompt=MARKDOWN_PROMPT)\n",
    "    os.remove(pages_filename)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequentially_process_pdf(filename, np=1):\n",
    "    # find total number of pages in the pdf document\n",
    "    total_pages = get_number_of_pages(filename)\n",
    "\n",
    "    results = []\n",
    "    skipped_pages = []\n",
    "    for i in range(0, total_pages, np):\n",
    "        print(f\"Processing pages {i} to {i+np}\")\n",
    "        pages_filename = get_initial_pages(filename, pmin=i, pmax=i+np)\n",
    "        failures = 0\n",
    "        success = False\n",
    "        result = None\n",
    "        while (not success) and (failures < 3):\n",
    "            try:\n",
    "                result = info_from_doc_template(filename=pages_filename, template=MARKDOWN_TEMPLATE, prompt=MARKDOWN_PROMPT)\n",
    "                success = True\n",
    "                time.sleep(5)\n",
    "            except:\n",
    "                print(f\"Error processing page {i+1}\")\n",
    "                failures += 1\n",
    "                print(f\"Retrying in 60 seconds.\")\n",
    "                time.sleep(60)\n",
    "                if failures == 3:\n",
    "                    result = {\n",
    "                        'markdown': \"**skipped**\",\n",
    "                    }\n",
    "                    print(f\"Failed to process page {i+1} after 3 attempts.\")\n",
    "                    skipped_pages.append(i+1)\n",
    "\n",
    "        results.append(result)\n",
    "        os.remove(pages_filename)\n",
    "    print(f\" Had to skip pages: {len(skipped_pages)}\")\n",
    "    return results, skipped_pages\n",
    "\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing pages 0 to 1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_initial_pages' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m file_path \u001b[38;5;241m=\u001b[39m PDF_LOCATION\n\u001b[0;32m----> 3\u001b[0m results, skipped_pages \u001b[38;5;241m=\u001b[39m \u001b[43msequentially_process_pdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 9\u001b[0m, in \u001b[0;36msequentially_process_pdf\u001b[0;34m(filename, np)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, total_pages, np):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing pages \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39mnp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m     pages_filename \u001b[38;5;241m=\u001b[39m \u001b[43mget_initial_pages\u001b[49m(filename, pmin\u001b[38;5;241m=\u001b[39mi, pmax\u001b[38;5;241m=\u001b[39mi\u001b[38;5;241m+\u001b[39mnp)\n\u001b[1;32m     10\u001b[0m     failures \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     11\u001b[0m     success \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_initial_pages' is not defined"
     ]
    }
   ],
   "source": [
    "file_path = PDF_LOCATION\n",
    "\n",
    "results, skipped_pages = sequentially_process_pdf(file_path, np=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "import pickle\n",
    "model = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "\n",
    "output_pickle_path = os.path.join(OUTPUT_FOLDER, f\"_{model}_pages.pkl\")\n",
    "\n",
    "documents = []\n",
    "for i, response in enumerate(results):\n",
    "    doc = Document(text=results[i]['markdown'], metadata={\"page_number\": i+1})\n",
    "    documents.append(doc)\n",
    "parsed_result = {\"parsed_documents\": documents, \"skipped_pages\": skipped_pages}\n",
    "pickle.dump(parsed_result, open(output_pickle_path, 'wb'))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filenames = [\n",
    "#     'IndustrySource/Misc/3D Printer Manufacturing in the US.pdf'\n",
    "# ]\n",
    "# markdown = [extract_markdown(filename, 45, 48) for filename in filenames[:5]]\n",
    "# markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2 : Load documents and do post processing to correct the heading structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "import pickle\n",
    "\n",
    "# PDF_LOCATION = 'IndustrySource/Misc/3D Printer Manufacturing in the US.pdf'\n",
    "PDF_LOCATION = 'IndustrySource/Misc/HVAC%20Service%20Franchises%20in%20the%20US.pdf'\n",
    "DOC_ID = PDF_LOCATION.split('/')[-1].split('.')[0].lower().replace(' ', '-')\n",
    "OUTPUT_FOLDER = f'./rag_outputs/{DOC_ID}'\n",
    "\n",
    "\n",
    "model = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "\n",
    "output_pickle_path = os.path.join(OUTPUT_FOLDER, f\"_{model}_pages.pkl\")\n",
    "print(output_pickle_path)\n",
    "# load pickle file if it exists\n",
    "if os.path.exists(output_pickle_path):\n",
    "    with open(output_pickle_path, 'rb') as f:\n",
    "        parsed_result = pickle.load(f)\n",
    "\n",
    "documents = parsed_result['parsed_documents']\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response_to_text(content_text: str, template: dict, main_prompt: str, system_prompt: str, final_prompt: str, tool_name: str=\"info_extract\") -> dict:\n",
    "    initial_message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"text\": main_prompt,\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    initial_message['content'].append({\"text\": content_text})\n",
    "    if final_prompt is not None:\n",
    "        initial_message['content'].append({\"text\": final_prompt})\n",
    "    \n",
    "\n",
    "    tool_list = [{\n",
    "        \"toolSpec\": template\n",
    "    }]\n",
    "    # model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "    model_id = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "    response = bedrock.converse(\n",
    "        modelId=model_id,\n",
    "        # modelId=\"meta.llama3-1-405b-instruct-v1:0\",\n",
    "        messages=[initial_message],\n",
    "        # system = system_prompt,\n",
    "        inferenceConfig={\n",
    "            \"temperature\": 0,\n",
    "        },\n",
    "        toolConfig={\n",
    "            \"tools\": tool_list,\n",
    "            \"toolChoice\": {\n",
    "                \"tool\": {\n",
    "                    \"name\": tool_name\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    core_response = response['output']['message']['content'][0]['toolUse']['input']\n",
    "    if 'properties' in core_response:\n",
    "        core_response: dict = core_response['properties']\n",
    "    for k, v in core_response.items():\n",
    "        if isinstance(v, str) and v[0] in '{[' and v[-1] in ']}':\n",
    "            try:\n",
    "                core_response[k] = json.loads(v)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    return core_response, response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [document.text for document in documents]\n",
    "# iterate over docs and see if the first sentence contains the title, then remove the whole line from the text\n",
    "all_sentences = []\n",
    "for i, doc in enumerate(docs):\n",
    "    sentences = doc.split(\"\\n\")\n",
    "    all_sentences.append(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_titles = []\n",
    "for i, sentences in enumerate(all_sentences):\n",
    "    page_headings = []\n",
    "    for j, sentence in enumerate(sentences):\n",
    "        # sentence is a heading if it starts with # or ## or ### or #### or #####\n",
    "        if sentence.startswith(\"#\"):\n",
    "            page_headings.append(sentence)\n",
    "    all_titles.append(page_headings)\n",
    "\n",
    "print(len(all_titles))\n",
    "all_titles[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAGE_TITLE_PROMPT = \"\"\" You are expert in determining the page numbers where a specific section begins from the list of titles and corresponding page numbers.\n",
    "There are 6 sections in the document: [\"Industry at a Glance\", \"Supply Chain\", \"Competitive Landscape\", \"Costs & Operations\", \"Questions for Owners\", \"Datatables & Glossary\"]\n",
    "Section 1: Industry at a Glance contains subsections such as Key Statistics, Executive Summary, Current Performance, Future Outlook, Industry Definition, Industry Impact, SWOT Analysis, Key Trends.\n",
    "Section 2: Supply Chain contains subsections such as External drivers, Supply Chain, Similar Industries, Related International Industries, Products & Services, Demand Determinants, Market Segmentation, Business Locations\n",
    "Section 3: Competitive Landscape contains subsections such as Basis of Competition, Barriers to Entry, Market Share Concentration, Industry Globalization\n",
    "Section 4: Costs & Operations contains subsections such as Cost Structure, Capital Intensity, Revenue Volatility, Regulation & Policy, Industry Assistance\n",
    "Section 5: Questions for Owners contains some questions and answers\n",
    "Section 6: Datatables & Glossary contains some tables for Industry Data and glossary of industry terms.\n",
    "You will be given a list of titles parsed from pdf and the corresponding page number where each title was parsed from. Each title begins in a new line with a page number as prefix enclosed in angle brackets <>.\n",
    "Use the given information and your best jusdgement to determine the page number where each section begins.\n",
    "Use the tool to output the page numbers in json format. If you are not sure about any section, output 0.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Templates.aws_templates_common import build_aws_template\n",
    "from Templates.aws_section_page_number_template import TEMPLATE as page_number_template\n",
    "\n",
    "def get_page_numbers_for_sections(all_titles: List[List[str]]):\n",
    "    template = page_number_template\n",
    "    template_parts = template['data']\n",
    "    page_title_template = build_aws_template(template_parts, tool_name=\"page_number_inference\")[0]\n",
    "\n",
    "    #add page number as prefix to each title\n",
    "    all_titles_with_page = []\n",
    "    for i, titles in enumerate(all_titles):\n",
    "        titles_with_page = [f\"<Page {i+1}> {title}\" for title in titles]\n",
    "        all_titles_with_page.extend(titles_with_page)\n",
    "    print(all_titles_with_page)\n",
    "    titles_text = \"\\n\".join(all_titles_with_page)\n",
    "    num_failed = 0\n",
    "    response = None\n",
    "    response_raw = None\n",
    "    while num_failed < 5:\n",
    "        try:     \n",
    "            response, response_raw = response_to_text(titles_text, page_title_template, PAGE_TITLE_PROMPT, None, None, tool_name=\"page_number_inference\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to get page numbers for sections. Retrying in 60 seconds.\")\n",
    "            print(e)\n",
    "            time.sleep(60)\n",
    "            num_failed += 1\n",
    "    return response, response_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_mapping_list(page_mapping, sections_names):\n",
    "    if page_mapping is None:\n",
    "        page_mapping = [0]*len(sections_names)\n",
    "    page_mapping_list = []\n",
    "\n",
    "    total_pages = get_number_of_pages(PDF_LOCATION)\n",
    "    for sec_ix, sec in enumerate(sections_names):\n",
    "        sec_mod = sec.lower().replace(' & ','_').replace(' ', '_')\n",
    "        f_p = page_mapping[sec_mod]\n",
    "        prev_ix = sec_ix-1\n",
    "        while f_p == 0 and prev_ix >= 0:\n",
    "            prev_sec = sections_names[prev_ix].lower().replace(' & ','_').replace(' ', '_')\n",
    "            f_p = page_mapping[prev_sec]\n",
    "            prev_ix -= 1   \n",
    "        if f_p == 0:\n",
    "            f_p = 1\n",
    "        \n",
    "        if sec_ix == len(sections_names)-1:\n",
    "            e_p = total_pages\n",
    "        else:\n",
    "            sec_next = sections_names[sec_ix+1].lower().replace(' & ','_').replace(' ', '_')\n",
    "            e_p = min(page_mapping[sec_next], total_pages)\n",
    "        next_ix = sec_ix+1\n",
    "        while e_p == 0 and next_ix < len(sections_names):\n",
    "            next_sec = sections_names[next_ix].lower().replace(' & ','_').replace(' ', '_')\n",
    "            e_p = page_mapping[next_sec]\n",
    "            next_ix += 1\n",
    "        if e_p == 0:\n",
    "            e_p = total_pages\n",
    "        page_mapping_list.append((sec, f_p, e_p))\n",
    "    return page_mapping_list\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3: Extract info from respective section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "INFO_EXTRACTION_PROMPT = \"\"\"\n",
    "You are an expert in extracting market and financial data from documents.\n",
    "Use the given tool to extract essential data from text in the enclosed document. Do not make any assumptions or add any information that is not present in the text.\n",
    "\n",
    "Return the result in JSON format. Do not use non-JSON tags. If some numeric data is not present in the text, simply output the number 101 as an answer where numeric data is expected.\n",
    "For titles and names, limit the output to 20 words. For descriptions and key points, limit the output to 50 words.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_section_text(docs, start_page, end_page):\n",
    "    section_text = []\n",
    "    for i in range(start_page-1, end_page):\n",
    "        section_text.append(docs[i])\n",
    "    return \"\\n\".join(section_text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_info_for_section(docs, template, main_prompt, start_page, end_page):\n",
    "    section_text = get_section_text(docs, start_page, end_page)\n",
    "    num_failed = 0\n",
    "    result = None\n",
    "    while num_failed < 5:\n",
    "        try:\n",
    "            result = response_to_text(section_text, template, main_prompt, None, None)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            num_failed += 1\n",
    "            print(f\"Error: {e}\")\n",
    "            print(f\"Failed {num_failed} times. Sleeping for 60 seconds.\")\n",
    "            time.sleep(60)\n",
    "            continue\n",
    "    return result\n",
    "\n",
    "def extract_info_for_all_sections(page_mapping_list, full_templates, docs):\n",
    "    section_summaries = []\n",
    "    raw_responses = []\n",
    "    for idx, elem in enumerate(page_mapping_list):\n",
    "        print(f\"Extracting for section {elem}\")\n",
    "        section_name, start_page, end_page = elem\n",
    "        if idx >= len(full_templates):\n",
    "            break\n",
    "        section_summary, raw_response = extract_info_for_section(docs, full_templates[idx], INFO_EXTRACTION_PROMPT, start_page, end_page)\n",
    "        section_summaries.append(section_summary)\n",
    "        raw_responses.append(raw_response)\n",
    "        print(f\"Extracted for section {section_name}\")\n",
    "        print(\"Sleeping for 60 seconds.\")\n",
    "        time.sleep(60)\n",
    "    return section_summaries, raw_responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections_names = [\"Industry at a Glance\", \"Supply Chain\", \"Competitive Landscape\", \"Costs & Operations\", \"Questions for Owners\", \"Datatables & Glossary\"]\n",
    "\n",
    "page_mapping, page_response_raw = get_page_numbers_for_sections(all_titles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(page_mapping)\n",
    "page_mapping_list = get_page_mapping_list(page_mapping, sections_names)\n",
    "print(page_mapping_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from Templates.ibis_aws_summary_template_all import TEMPLATE as IBIS_SUMMARY_TEMPLATE\n",
    "\n",
    "\n",
    "template_parts = IBIS_SUMMARY_TEMPLATE['data']\n",
    "full_templates = build_aws_template(template_parts)\n",
    "\n",
    "pickle.dump((page_mapping, page_response_raw), open(f\"{OUTPUT_FOLDER}/page_mappings.pkl\", \"wb\"))\n",
    "# print(full_templates)\n",
    "section_summaries, raw_responses = extract_info_for_all_sections(page_mapping_list, full_templates, docs)\n",
    "\n",
    "section_results = {\"section_summaries\": section_summaries, \"raw_responses\": raw_responses}\n",
    "\n",
    "pickle.dump(section_results, open(f\"{OUTPUT_FOLDER}/section_summaries.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_summaries[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from build_markdown_report import build_markdown_report_func\n",
    "\n",
    "report_md = build_markdown_report_func(section_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(report_md)\n",
    "# save the markdown report\n",
    "with open(f\"{OUTPUT_FOLDER}/summary_report.md\", \"w\") as file:\n",
    "    file.write(report_md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Templates.build_markdown_report import report_order\n",
    "import json\n",
    "json_path = f\"{OUTPUT_FOLDER}/section_summaries.json\"\n",
    "with open(json_path, \"w\") as f:\n",
    "    json.dump(section_summaries, f, indent=4)\n",
    "\n",
    "report_order_json_path = f\"{OUTPUT_FOLDER}/report_order.json\"\n",
    "with open(report_order_json_path, \"w\") as f:\n",
    "    json.dump(report_order, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "probe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
